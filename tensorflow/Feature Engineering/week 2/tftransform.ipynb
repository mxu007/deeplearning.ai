{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h1> Exploring tf.transform </h1>\n",
    "\n",
    "While Pandas is fine for experimenting, for operationalization of your workflow, it is better to do preprocessing in Apache Beam. This will also help if you need to preprocess data in flight, since Apache Beam also allows for streaming.\n",
    "\n",
    "Only specific combinations of TensorFlow/Beam are supported by tf.transform. So make sure to get a combo that is.\n",
    "\n",
    "* TFT 0.6.0\n",
    "* TF 1.6 or higher\n",
    "* Apache Beam [GCP] 2.4.0 or higher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling google-cloud-dataflow-2.0.0:\n",
      "  Successfully uninstalled google-cloud-dataflow-2.0.0\n",
      "Collecting tensorflow_transform==0.6.0\n",
      "  Downloading https://files.pythonhosted.org/packages/a4/e5/b1f69575d78936acc2a41344dadf753c005025e20f03afe38c43e08cc8f2/tensorflow-transform-0.6.0.tar.gz (113kB)\n",
      "Collecting apache-beam[gcp]\n",
      "  Downloading https://files.pythonhosted.org/packages/d6/7d/1294f67c4132e4385edbc6c9efb4f83a7262fa45321e3299c56aaf238de1/apache-beam-2.6.0.zip (2.3MB)\n",
      "Collecting numpy<2,>=1.10 (from tensorflow_transform==0.6.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/85/51/ba4564ded90e093dbb6adfc3e21f99ae953d9ad56477e1b0d4a93bacf7d3/numpy-1.15.0-cp27-cp27mu-manylinux1_x86_64.whl (13.8MB)\n",
      "Collecting protobuf<4,>=3.5.2 (from tensorflow_transform==0.6.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/27/e7/bf96130ebe633b08a3913da4bb25e50dac5779f1f68e51c99485423f7443/protobuf-3.6.0-cp27-cp27mu-manylinux1_x86_64.whl (7.1MB)\n",
      "Collecting six<2,>=1.9 (from tensorflow_transform==0.6.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/67/4b/141a581104b1f6397bfa78ac9d43d8ad29a7ca43ea90a2d863fe3056e86a/six-1.11.0-py2.py3-none-any.whl\n",
      "Collecting avro<2.0.0,>=1.8.1 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/eb/27/143f124a7498f841317a92ced877150c5cb8d28a4109ec39666485925d00/avro-1.8.2.tar.gz (43kB)\n",
      "Collecting crcmod<2.0,>=1.7 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/6b/b0/e595ce2a2527e169c3bcd6c33d2473c1918e0b7f6826a043ca1245dd4e5b/crcmod-1.7.tar.gz (89kB)\n",
      "Collecting dill<=0.2.8.2,>=0.2.6 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/6f/78/8b96476f4ae426db71c6e86a8e6a81407f015b34547e442291cd397b18f3/dill-0.2.8.2.tar.gz (150kB)\n",
      "Collecting grpcio<2,>=1.8 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/b5/84/c0d0a0355f2e3ea1e49fd81aa123e0bf42bfaa58be56583cc3b9baaf2837/grpcio-1.14.1-cp27-cp27mu-manylinux1_x86_64.whl (9.2MB)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/bb/ef/35af4764ea6c60bd14195ca78d4e831d154183f35cc4af4a0b3e01aa28ce/hdfs-2.1.0.tar.gz\n",
      "Collecting httplib2<=0.11.3,>=0.8 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/fd/ce/aa4a385e3e9fd351737fd2b07edaa56e7a730448465aceda6b35086a0d9b/httplib2-0.11.3.tar.gz (215kB)\n",
      "Collecting mock<3.0.0,>=1.0.1 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
      "Collecting oauth2client<5,>=2.0.1 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/82/d8/3eab58811282ac7271a081ba5c0d4b875ce786ca68ce43e2a62ade32e9a8/oauth2client-4.1.2-py2.py3-none-any.whl (99kB)\n",
      "Collecting pydot<1.3,>=1.2.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/c3/f1/e61d6dfe6c1768ed2529761a68f70939e2569da043e9f15a8d84bf56cadf/pydot-1.2.4.tar.gz (132kB)\n",
      "Collecting pytz<=2018.4,>=2018.3 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/dc/83/15f7833b70d3e067ca91467ca245bae0f6fe56ddc7451aa0dc5606b120f2/pytz-2018.4-py2.py3-none-any.whl (510kB)\n",
      "Collecting pyyaml<4.0.0,>=3.12 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/9e/a3/1d13970c3f36777c583f136c136f804d70f500168edc1edea6daa7200769/PyYAML-3.13.tar.gz (270kB)\n",
      "Collecting pyvcf<0.7.0,>=0.6.8 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/20/b6/36bfb1760f6983788d916096193fc14c83cce512c7787c93380e09458c09/PyVCF-0.6.8.tar.gz\n",
      "Collecting typing<3.7.0,>=3.6.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/0d/4d/4e5985d075d241d686a1663fa1f88b61d544658d08c1375c7c6aac32afc3/typing-3.6.4-py2-none-any.whl\n",
      "Collecting futures<4.0.0,>=3.1.1 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/2d/99/b2c4e9d5a30f6471e410a146232b4118e697fa3ffc06d6a65efde84debd0/futures-3.2.0-py2-none-any.whl\n",
      "Collecting future<1.0.0,>=0.16.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/00/2b/8d082ddfed935f3608cc61140df6dcbf0edea1bc3ab52fb6c29ae3e81e85/future-0.16.0.tar.gz (824kB)\n",
      "Collecting fastavro==0.19.7 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/a7/0a/b08ba5cef63c675e8442c2bf1cbcef90c8b9f824be2202d492f0cedb0913/fastavro-0.19.7.tar.gz (499kB)\n",
      "Collecting google-apitools<=0.5.20,>=0.5.18 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/1d/0c/64f84f91643f775fdb64c6c10f4a4f0d827f8b0d98a2ba2b4bb9dc2f8646/google_apitools-0.5.20-py2-none-any.whl (330kB)\n",
      "Collecting proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/2a/1f/4124f15e1132a2eeeaf616d825990bb1d395b4c2c37362654ea5cd89bb42/proto-google-cloud-datastore-v1-0.90.4.tar.gz\n",
      "Collecting googledatastore==7.0.1 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/73/d0/17ce873331aaf529ab238464a15fd7bdc1ba8d2c684789970a7fa8b505a8/googledatastore-7.0.1.tar.gz\n",
      "Collecting google-cloud-pubsub==0.26.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/37/92/c74a643126d58505daec9addf872dfaffea3305981b90cc435f4b9213cdd/google_cloud_pubsub-0.26.0-py2.py3-none-any.whl\n",
      "Collecting proto-google-cloud-pubsub-v1==0.15.4 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/c0/a2/2eeffa0069830f00016196dfdd69491cf562372b5353f2e8e378b3c2cb0a/proto-google-cloud-pubsub-v1-0.15.4.tar.gz\n",
      "Collecting google-cloud-bigquery==0.25.0 (from apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/76/67/6165c516ff6ceaa62eb61f11d8451e1b0acc4d3775e181630aba9652babb/google_cloud_bigquery-0.25.0-py2.py3-none-any.whl (41kB)\n",
      "Collecting setuptools (from protobuf<4,>=3.5.2->tensorflow_transform==0.6.0)\n",
      "  Downloading https://files.pythonhosted.org/packages/ff/f4/385715ccc461885f3cedf57a41ae3c12b5fec3f35cce4c8706b1a112a133/setuptools-40.0.0-py2.py3-none-any.whl (567kB)\n",
      "Collecting enum34>=1.0.4 (from grpcio<2,>=1.8->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/c5/db/e56e6b4bbac7c4a06de1c50de6fe1ef3810018ae11732a50f15f62c7d050/enum34-1.1.6-py2-none-any.whl\n",
      "Collecting docopt (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/a2/55/8f8cab2afd404cf578136ef2cc5dfb50baa1761b68c9da1fb1e4eed343c9/docopt-0.6.2.tar.gz\n",
      "Collecting requests>=2.7.0 (from hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/65/47/7e02164a2a3db50ed6d8a6ab1d6d60b69c4c3fdf57a284257925dfc12bda/requests-2.19.1-py2.py3-none-any.whl (91kB)\n",
      "Collecting funcsigs>=1; python_version < \"3.3\" (from mock<3.0.0,>=1.0.1->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/69/cb/f5be453359271714c01b9bd06126eaf2e368f1fddfff30818754b5ac2328/funcsigs-1.0.2-py2.py3-none-any.whl\n",
      "Collecting pbr>=0.11 (from mock<3.0.0,>=1.0.1->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/69/1c/98cba002ed975a91a0294863d9c774cc0ebe38e05bbb65e83314550b1677/pbr-4.2.0-py2.py3-none-any.whl (100kB)\n",
      "Collecting rsa>=3.1.4 (from oauth2client<5,>=2.0.1->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
      "Collecting pyasn1>=0.1.7 (from oauth2client<5,>=2.0.1->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/d1/a1/7790cc85db38daa874f6a2e6308131b9953feb1367f2ae2d1123bb93a9f5/pyasn1-0.4.4-py2.py3-none-any.whl (72kB)\n",
      "Collecting pyasn1-modules>=0.0.5 (from oauth2client<5,>=2.0.1->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/19/02/fa63f7ba30a0d7b925ca29d034510fc1ffde53264b71b4155022ddf3ab5d/pyasn1_modules-0.2.2-py2.py3-none-any.whl (62kB)\n",
      "Collecting pyparsing>=2.1.4 (from pydot<1.3,>=1.2.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/8a/718fd7d3458f9fab8e67186b00abdd345b639976bc7fb3ae722e1b026a50/pyparsing-2.2.0-py2.py3-none-any.whl (56kB)\n",
      "Collecting fasteners>=0.14 (from google-apitools<=0.5.20,>=0.5.18->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/14/3a/096c7ad18e102d4f219f5dd15951f9728ca5092a3385d2e8f79a7c1e1017/fasteners-0.14.1-py2.py3-none-any.whl\n",
      "Collecting googleapis-common-protos<2.0dev,>=1.5.2 (from proto-google-cloud-datastore-v1<=0.90.4,>=0.90.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/00/03/d25bed04ec8d930bcfa488ba81a2ecbf7eb36ae3ffd7e8f5be0d036a89c9/googleapis-common-protos-1.5.3.tar.gz\n",
      "Collecting google-cloud-core<0.26dev,>=0.25.0 (from google-cloud-pubsub==0.26.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/ef/dd/00e90bd1f6788f06ca5ea83a0ec8dd76350b38303bb8f09d2bf692eb1294/google_cloud_core-0.25.0-py2.py3-none-any.whl (52kB)\n",
      "Collecting gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0 (from google-cloud-pubsub==0.26.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a7/0225bd7a95e037a0afa90b2dd9534d0c79cd62283a5bddb30a3197579cbc/gapic-google-cloud-pubsub-v1-0.15.4.tar.gz\n",
      "Collecting idna<2.8,>=2.5 (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/4b/2a/0276479a4b3caeb8a8c1af2f8e4355746a97fab05a372e4a2c6a6b876165/idna-2.7-py2.py3-none-any.whl (58kB)\n",
      "Collecting chardet<3.1.0,>=3.0.2 (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/bc/a9/01ffebfb562e4274b6487b4bb1ddec7ca55ec7510b22e4c51f14098443b8/chardet-3.0.4-py2.py3-none-any.whl (133kB)\n",
      "Collecting urllib3<1.24,>=1.21.1 (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/bd/c9/6fdd990019071a4a32a5e7cb78a1d92c53851ef4f56f62a3486e6a7d8ffb/urllib3-1.23-py2.py3-none-any.whl (133kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.7.0->hdfs<3.0.0,>=2.1.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/7c/e6/92ad559b7192d846975fc916b65f667c7b8c3a32bea7372340bfe9a15fa5/certifi-2018.4.16-py2.py3-none-any.whl (150kB)\n",
      "Collecting monotonic>=0.1 (from fasteners>=0.14->google-apitools<=0.5.20,>=0.5.18->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/ac/aa/063eca6a416f397bd99552c534c6d11d57f58f2e94c14780f3bbf818c4cf/monotonic-1.5-py2.py3-none-any.whl\n",
      "Collecting google-auth-httplib2 (from google-cloud-core<0.26dev,>=0.25.0->google-cloud-pubsub==0.26.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/33/49/c814d6d438b823441552198f096fcd0377fd6c88714dbed34f1d3c8c4389/google_auth_httplib2-0.0.3-py2.py3-none-any.whl\n",
      "Collecting google-auth<2.0.0dev,>=0.4.0 (from google-cloud-core<0.26dev,>=0.25.0->google-cloud-pubsub==0.26.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/58/cb/96dbb4e50e7a9d856e89cc9c8e36ab1055f9774f7d85f37e2156c1d79d9f/google_auth-1.5.1-py2.py3-none-any.whl (65kB)\n",
      "Collecting google-gax<0.16dev,>=0.15.7 (from gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0->google-cloud-pubsub==0.26.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/4f/b4/ff312fa42f91535c67567c1d08e972db0e7c548e9a63c6f3bcc5213b32fc/google_gax-0.15.16-py2.py3-none-any.whl (46kB)\n",
      "Collecting grpc-google-iam-v1<0.12dev,>=0.11.1 (from gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0->google-cloud-pubsub==0.26.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/9b/28/f26f67381cb23e81271b8d66c00a846ad9d25a909ae1ae1df8222fad2744/grpc-google-iam-v1-0.11.4.tar.gz\n",
      "Collecting cachetools>=2.0.0 (from google-auth<2.0.0dev,>=0.4.0->google-cloud-core<0.26dev,>=0.25.0->google-cloud-pubsub==0.26.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/0a/58/cbee863250b31d80f47401d04f34038db6766f95dea1cc909ea099c7e571/cachetools-2.1.0-py2.py3-none-any.whl\n",
      "Collecting ply==3.8 (from google-gax<0.16dev,>=0.15.7->gapic-google-cloud-pubsub-v1<0.16dev,>=0.15.0->google-cloud-pubsub==0.26.0->apache-beam[gcp])\n",
      "  Downloading https://files.pythonhosted.org/packages/96/e0/430fcdb6b3ef1ae534d231397bee7e9304be14a47a267e82ebcb3323d0b5/ply-3.8.tar.gz (157kB)\n",
      "Building wheels for collected packages: tensorflow-transform, apache-beam, avro, crcmod, dill, hdfs, httplib2, pydot, pyyaml, pyvcf, future, fastavro, proto-google-cloud-datastore-v1, googledatastore, proto-google-cloud-pubsub-v1, docopt, googleapis-common-protos, gapic-google-cloud-pubsub-v1, grpc-google-iam-v1, ply\n",
      "  Running setup.py bdist_wheel for tensorflow-transform: started\n",
      "  Running setup.py bdist_wheel for tensorflow-transform: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/67/67/f9/a5f81a500bbbddf2822eb4371681ea21f6ca4ac16d94e4fcdf\n",
      "  Running setup.py bdist_wheel for apache-beam: started\n",
      "  Running setup.py bdist_wheel for apache-beam: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/33/53/aa/f7d4a2e946b48a6e03baaf0b9a147c0015fcd396bd37d66b07\n",
      "  Running setup.py bdist_wheel for avro: started\n",
      "  Running setup.py bdist_wheel for avro: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/bf/0b/75/1b3517b7d36ddc8ba5d22c0df5eb01e83979f34420066d643e\n",
      "  Running setup.py bdist_wheel for crcmod: started\n",
      "  Running setup.py bdist_wheel for crcmod: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/50/24/4d/4580ca4a299f1ad6fd63443e6e584cb21e9a07988e4aa8daac\n",
      "  Running setup.py bdist_wheel for dill: started\n",
      "  Running setup.py bdist_wheel for dill: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/e2/5d/17/f87cb7751896ac629b435a8696f83ee75b11029f5d6f6bda72\n",
      "  Running setup.py bdist_wheel for hdfs: started\n",
      "  Running setup.py bdist_wheel for hdfs: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/8c/89/59/b95e6bf2d5957541862f2cd109a052f26b61dc2fad2f04a1b4\n",
      "  Running setup.py bdist_wheel for httplib2: started\n",
      "  Running setup.py bdist_wheel for httplib2: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/1b/9c/9e/1f6fdb21dbb1fe6a99101d697f12cb8c1fa96c1587df69adba\n",
      "  Running setup.py bdist_wheel for pydot: started\n",
      "  Running setup.py bdist_wheel for pydot: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/6a/a5/14/25541ebcdeaf97a37b6d05c7ff15f5bd20f5e91b99d313e5b4\n",
      "  Running setup.py bdist_wheel for pyyaml: started\n",
      "  Running setup.py bdist_wheel for pyyaml: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/ad/da/0c/74eb680767247273e2cf2723482cb9c924fe70af57c334513f\n",
      "  Running setup.py bdist_wheel for pyvcf: started\n",
      "  Running setup.py bdist_wheel for pyvcf: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/81/91/41/3272543c0b9c61da9c525f24ee35bae6fe8f60d4858c66805d\n",
      "  Running setup.py bdist_wheel for future: started\n",
      "  Running setup.py bdist_wheel for future: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/bf/c9/a3/c538d90ef17cf7823fa51fc701a7a7a910a80f6a405bf15b1a\n",
      "  Running setup.py bdist_wheel for fastavro: started\n",
      "  Running setup.py bdist_wheel for fastavro: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/eb/5a/3e/3c2f0642da6a98b89f8ba910caee749af59a1ee4f1cde02f5b\n",
      "  Running setup.py bdist_wheel for proto-google-cloud-datastore-v1: started\n",
      "  Running setup.py bdist_wheel for proto-google-cloud-datastore-v1: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/bd/ce/33/8b769968db3761c42c7a91d8a0dbbafc50acfa0750866c8abd\n",
      "  Running setup.py bdist_wheel for googledatastore: started\n",
      "  Running setup.py bdist_wheel for googledatastore: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/93/fa/12/99c490525fff40bcd240504f694bd02848479bc2debc31daae\n",
      "  Running setup.py bdist_wheel for proto-google-cloud-pubsub-v1: started\n",
      "  Running setup.py bdist_wheel for proto-google-cloud-pubsub-v1: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/d1/0d/9e/95e7192ab2625847ac40b2bc618800bf5b6c984cd572a83314\n",
      "  Running setup.py bdist_wheel for docopt: started\n",
      "  Running setup.py bdist_wheel for docopt: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/9b/04/dd/7daf4150b6d9b12949298737de9431a324d4b797ffd63f526e\n",
      "  Running setup.py bdist_wheel for googleapis-common-protos: started\n",
      "  Running setup.py bdist_wheel for googleapis-common-protos: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/62/45/af/649bbf07b6595fda010be1bda667cd56d0444d07afc6f8b687\n",
      "  Running setup.py bdist_wheel for gapic-google-cloud-pubsub-v1: started\n",
      "  Running setup.py bdist_wheel for gapic-google-cloud-pubsub-v1: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/f4/2b/10/bdcbc9be2ae4e437232e118056f026025cf2cc46d6dcf0d69d\n",
      "  Running setup.py bdist_wheel for grpc-google-iam-v1: started\n",
      "  Running setup.py bdist_wheel for grpc-google-iam-v1: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/b6/c6/31/c20321a5a3fde456fc375b7c2814135e6e98bc0d74c40239d9\n",
      "  Running setup.py bdist_wheel for ply: started\n",
      "  Running setup.py bdist_wheel for ply: finished with status 'done'\n",
      "  Stored in directory: /content/.cache/pip/wheels/f2/21/c0/f0056cc96847933daa961a19eb59a2ecd0228fdbe3376e7a68\n",
      "Successfully built tensorflow-transform apache-beam avro crcmod dill hdfs httplib2 pydot pyyaml pyvcf future fastavro proto-google-cloud-datastore-v1 googledatastore proto-google-cloud-pubsub-v1 docopt googleapis-common-protos gapic-google-cloud-pubsub-v1 grpc-google-iam-v1 ply\n",
      "Installing collected packages: avro, crcmod, dill, futures, enum34, six, grpcio, docopt, idna, chardet, urllib3, certifi, requests, hdfs, httplib2, funcsigs, pbr, mock, pyasn1, rsa, pyasn1-modules, oauth2client, setuptools, protobuf, pyparsing, pydot, pytz, pyyaml, pyvcf, typing, future, fastavro, monotonic, fasteners, google-apitools, googleapis-common-protos, proto-google-cloud-datastore-v1, googledatastore, cachetools, google-auth, google-auth-httplib2, google-cloud-core, ply, google-gax, proto-google-cloud-pubsub-v1, grpc-google-iam-v1, gapic-google-cloud-pubsub-v1, google-cloud-pubsub, google-cloud-bigquery, apache-beam, numpy, tensorflow-transform\n",
      "  Found existing installation: avro 1.8.2\n",
      "    Uninstalling avro-1.8.2:\n",
      "      Successfully uninstalled avro-1.8.2\n",
      "  Found existing installation: crcmod 1.7\n",
      "    Uninstalling crcmod-1.7:\n",
      "      Successfully uninstalled crcmod-1.7\n",
      "  Found existing installation: dill 0.2.6\n",
      "    Uninstalling dill-0.2.6:\n",
      "      Successfully uninstalled dill-0.2.6\n",
      "  Found existing installation: futures 3.2.0\n",
      "    Uninstalling futures-3.2.0:\n",
      "      Successfully uninstalled futures-3.2.0\n",
      "  Found existing installation: enum34 1.1.6\n",
      "    Uninstalling enum34-1.1.6:\n",
      "      Successfully uninstalled enum34-1.1.6\n",
      "  Found existing installation: six 1.10.0\n",
      "    Uninstalling six-1.10.0:\n",
      "      Successfully uninstalled six-1.10.0\n",
      "  Found existing installation: grpcio 1.13.0\n",
      "    Uninstalling grpcio-1.13.0:\n",
      "      Successfully uninstalled grpcio-1.13.0\n",
      "  Found existing installation: idna 2.6\n",
      "    Uninstalling idna-2.6:\n",
      "      Successfully uninstalled idna-2.6\n",
      "  Found existing installation: chardet 3.0.4\n",
      "    Uninstalling chardet-3.0.4:\n",
      "      Successfully uninstalled chardet-3.0.4\n",
      "  Found existing installation: urllib3 1.22\n",
      "    Uninstalling urllib3-1.22:\n",
      "      Successfully uninstalled urllib3-1.22\n",
      "  Found existing installation: certifi 2018.4.16\n",
      "    Uninstalling certifi-2018.4.16:\n",
      "      Successfully uninstalled certifi-2018.4.16\n",
      "  Found existing installation: requests 2.18.4\n",
      "    Uninstalling requests-2.18.4:\n",
      "      Successfully uninstalled requests-2.18.4\n",
      "  Found existing installation: httplib2 0.11.3\n",
      "    Uninstalling httplib2-0.11.3:\n",
      "      Successfully uninstalled httplib2-0.11.3\n",
      "  Found existing installation: funcsigs 1.0.0\n",
      "    Uninstalling funcsigs-1.0.0:\n",
      "      Successfully uninstalled funcsigs-1.0.0\n",
      "  Found existing installation: pbr 4.0.4\n",
      "    Uninstalling pbr-4.0.4:\n",
      "      Successfully uninstalled pbr-4.0.4\n",
      "  Found existing installation: mock 2.0.0\n",
      "    Uninstalling mock-2.0.0:\n",
      "      Successfully uninstalled mock-2.0.0\n",
      "  Found existing installation: pyasn1 0.4.3\n",
      "    Uninstalling pyasn1-0.4.3:\n",
      "      Successfully uninstalled pyasn1-0.4.3\n",
      "  Found existing installation: rsa 3.4.2\n",
      "    Uninstalling rsa-3.4.2:\n",
      "      Successfully uninstalled rsa-3.4.2\n",
      "  Found existing installation: pyasn1-modules 0.2.2\n",
      "    Uninstalling pyasn1-modules-0.2.2:\n",
      "      Successfully uninstalled pyasn1-modules-0.2.2\n",
      "  Found existing installation: oauth2client 2.2.0\n",
      "    Uninstalling oauth2client-2.2.0:\n",
      "      Successfully uninstalled oauth2client-2.2.0\n",
      "  Found existing installation: setuptools 39.2.0\n",
      "    Uninstalling setuptools-39.2.0:\n",
      "      Successfully uninstalled setuptools-39.2.0\n",
      "  Found existing installation: protobuf 3.5.2\n",
      "    Uninstalling protobuf-3.5.2:\n",
      "      Successfully uninstalled protobuf-3.5.2\n",
      "  Found existing installation: pyparsing 2.2.0\n",
      "    Uninstalling pyparsing-2.2.0:\n",
      "      Successfully uninstalled pyparsing-2.2.0\n",
      "  Found existing installation: pytz 2016.7\n",
      "    Uninstalling pytz-2016.7:\n",
      "      Successfully uninstalled pytz-2016.7\n",
      "  Found existing installation: PyYAML 3.12\n",
      "    Uninstalling PyYAML-3.12:\n",
      "      Successfully uninstalled PyYAML-3.12\n",
      "  Found existing installation: future 0.16.0\n",
      "    Uninstalling future-0.16.0:\n",
      "      Successfully uninstalled future-0.16.0\n",
      "  Found existing installation: google-apitools 0.5.10\n",
      "    Uninstalling google-apitools-0.5.10:\n",
      "      Successfully uninstalled google-apitools-0.5.10\n",
      "  Found existing installation: googleapis-common-protos 1.5.3\n",
      "    Uninstalling googleapis-common-protos-1.5.3:\n",
      "      Successfully uninstalled googleapis-common-protos-1.5.3\n",
      "  Found existing installation: proto-google-cloud-datastore-v1 0.90.4\n",
      "    Uninstalling proto-google-cloud-datastore-v1-0.90.4:\n",
      "      Successfully uninstalled proto-google-cloud-datastore-v1-0.90.4\n",
      "  Found existing installation: googledatastore 7.0.1\n",
      "    Uninstalling googledatastore-7.0.1:\n",
      "      Successfully uninstalled googledatastore-7.0.1\n",
      "  Found existing installation: cachetools 2.0.1\n",
      "    Uninstalling cachetools-2.0.1:\n",
      "      Successfully uninstalled cachetools-2.0.1\n",
      "  Found existing installation: google-auth 1.5.0\n",
      "    Uninstalling google-auth-1.5.0:\n",
      "      Successfully uninstalled google-auth-1.5.0\n",
      "  Found existing installation: google-auth-httplib2 0.0.3\n",
      "    Uninstalling google-auth-httplib2-0.0.3:\n",
      "      Successfully uninstalled google-auth-httplib2-0.0.3\n",
      "  Found existing installation: google-cloud-core 0.28.1\n",
      "    Uninstalling google-cloud-core-0.28.1:\n",
      "      Successfully uninstalled google-cloud-core-0.28.1\n",
      "  Found existing installation: ply 3.8\n",
      "    Uninstalling ply-3.8:\n",
      "      Successfully uninstalled ply-3.8\n",
      "  Found existing installation: google-gax 0.15.16\n",
      "    Uninstalling google-gax-0.15.16:\n",
      "      Successfully uninstalled google-gax-0.15.16\n",
      "  Found existing installation: grpc-google-iam-v1 0.11.4\n",
      "    Uninstalling grpc-google-iam-v1-0.11.4:\n",
      "      Successfully uninstalled grpc-google-iam-v1-0.11.4\n",
      "  Found existing installation: google-cloud-pubsub 0.30.1\n",
      "    Uninstalling google-cloud-pubsub-0.30.1:\n",
      "      Successfully uninstalled google-cloud-pubsub-0.30.1\n",
      "  Found existing installation: google-cloud-bigquery 0.28.0\n",
      "    Uninstalling google-cloud-bigquery-0.28.0:\n",
      "      Successfully uninstalled google-cloud-bigquery-0.28.0\n",
      "  Found existing installation: numpy 1.14.0\n",
      "    Uninstalling numpy-1.14.0:\n",
      "      Successfully uninstalled numpy-1.14.0\n",
      "Successfully installed apache-beam-2.6.0 avro-1.8.2 cachetools-2.1.0 certifi-2018.4.16 chardet-3.0.4 crcmod-1.7 dill-0.2.8.2 docopt-0.6.2 enum34-1.1.6 fastavro-0.19.7 fasteners-0.14.1 funcsigs-1.0.2 future-0.16.0 futures-3.2.0 gapic-google-cloud-pubsub-v1-0.15.4 google-apitools-0.5.20 google-auth-1.5.1 google-auth-httplib2-0.0.3 google-cloud-bigquery-0.25.0 google-cloud-core-0.25.0 google-cloud-pubsub-0.26.0 google-gax-0.15.16 googleapis-common-protos-1.5.3 googledatastore-7.0.1 grpc-google-iam-v1-0.11.4 grpcio-1.14.1 hdfs-2.1.0 httplib2-0.11.3 idna-2.7 mock-2.0.0 monotonic-1.5 numpy-1.15.0 oauth2client-4.1.2 pbr-4.2.0 ply-3.8 proto-google-cloud-datastore-v1-0.90.4 proto-google-cloud-pubsub-v1-0.15.4 protobuf-3.6.0 pyasn1-0.4.4 pyasn1-modules-0.2.2 pydot-1.2.4 pyparsing-2.2.0 pytz-2018.4 pyvcf-0.6.8 pyyaml-3.13 requests-2.19.1 rsa-3.4.2 setuptools-40.0.0 six-1.11.0 tensorflow-transform-0.6.0 typing-3.6.4 urllib3-1.23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n",
      "    DEPRECATION: Uninstalling a distutils installed project (crcmod) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n",
      "    DEPRECATION: Uninstalling a distutils installed project (dill) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n",
      "    DEPRECATION: Uninstalling a distutils installed project (certifi) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n",
      "    DEPRECATION: Uninstalling a distutils installed project (pytz) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n",
      "    DEPRECATION: Uninstalling a distutils installed project (pyyaml) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n",
      "    DEPRECATION: Uninstalling a distutils installed project (future) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n",
      "You are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "pip uninstall -y google-cloud-dataflow\n",
    "pip install --upgrade --force tensorflow_transform==0.6.0 apache-beam[gcp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<b>Restart the kernel</b> after you do a pip install (click on the <b>Reset</b> button in Datalab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache-airflow==1.9.0\n",
      "apache-beam==2.6.0\n",
      "tensorflow==1.8.0\n",
      "tensorflow-transform==0.6.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using pip version 9.0.3, however version 18.0 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "pip freeze | grep -e 'flow\\|beam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/__init__.py:45: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import h5a, h5d, h5ds, h5f, h5fd, h5g, h5r, h5s, h5t, h5p, h5z\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/h5py/_hl/group.py:22: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .. import h5g, h5i, h5o, h5r, h5t, h5l, h5p\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/scipy/ndimage/measurements.py:36: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from . import _ni_label\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/sklearn/utils/__init__.py:10: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .murmurhash import murmurhash3_32\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/envs/py2env/lib/python2.7/site-packages/sklearn/utils/extmath.py:24: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from ._logistic_sigmoid import _log_logistic_sigmoid\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/sklearn/utils/extmath.py:26: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .sparsefuncs_fast import csr_row_norms\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/sklearn/metrics/cluster/supervised.py:23: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .expected_mutual_info_fast import expected_mutual_information\n",
      "/usr/local/envs/py2env/lib/python2.7/site-packages/sklearn/metrics/pairwise.py:30: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  from .pairwise_fast import _chi2_kernel_fast, _sparse_manhattan\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_transform as tft\n",
    "import shutil\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# change these to try this notebook out\n",
    "BUCKET = 'qwiklabs-gcp-456f1b6ac3ab8d49'\n",
    "PROJECT = 'qwiklabs-gcp-456f1b6ac3ab8d49'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['BUCKET'] = BUCKET\n",
    "os.environ['PROJECT'] = PROJECT\n",
    "os.environ['REGION'] = REGION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n",
      "Updated property [compute/region].\n"
     ]
    }
   ],
   "source": [
    "%bash\n",
    "gcloud config set project $PROJECT\n",
    "gcloud config set compute/region $REGION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Input source: BigQuery\n",
    "\n",
    "Get data from BigQuery but defer filtering etc. to Beam.\n",
    "Note that the dayofweek column is now strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import google.datalab.bigquery as bq\n",
    "def create_query(phase, EVERY_N):\n",
    "  \"\"\"\n",
    "  phase: 1=train 2=valid\n",
    "  \"\"\"\n",
    "  base_query = \"\"\"\n",
    "WITH daynames AS\n",
    "  (SELECT ['Sun', 'Mon', 'Tues', 'Wed', 'Thurs', 'Fri', 'Sat'] AS daysofweek)\n",
    "SELECT\n",
    "  (tolls_amount + fare_amount) AS fare_amount,\n",
    "  daysofweek[ORDINAL(EXTRACT(DAYOFWEEK FROM pickup_datetime))] AS dayofweek,\n",
    "  EXTRACT(HOUR FROM pickup_datetime) AS hourofday,\n",
    "  pickup_longitude AS pickuplon,\n",
    "  pickup_latitude AS pickuplat,\n",
    "  dropoff_longitude AS dropofflon,\n",
    "  dropoff_latitude AS dropofflat,\n",
    "  passenger_count AS passengers,\n",
    "  'notneeded' AS key\n",
    "FROM\n",
    "  `nyc-tlc.yellow.trips`, daynames\n",
    "WHERE\n",
    "  trip_distance > 0 AND fare_amount > 0\n",
    "  \"\"\"\n",
    "\n",
    "  if EVERY_N == None:\n",
    "    if phase < 2:\n",
    "      # training\n",
    "      query = \"{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)),4) < 2\".format(base_query)\n",
    "    else:\n",
    "      query = \"{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING)),4) = {1}\".format(base_query, phase)\n",
    "  else:\n",
    "      query = \"{0} AND MOD(ABS(FARM_FINGERPRINT(CAST(pickup_datetime AS STRING))),{1}) = {2}\".format(base_query, EVERY_N, phase)\n",
    "    \n",
    "  return query\n",
    "\n",
    "query = create_query(2, 100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fare_amount</th>\n",
       "      <th>hourofday</th>\n",
       "      <th>pickuplon</th>\n",
       "      <th>pickuplat</th>\n",
       "      <th>dropofflon</th>\n",
       "      <th>dropofflat</th>\n",
       "      <th>passengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>11181.000000</td>\n",
       "      <td>11181.000000</td>\n",
       "      <td>11181.000000</td>\n",
       "      <td>11181.000000</td>\n",
       "      <td>11181.000000</td>\n",
       "      <td>11181.000000</td>\n",
       "      <td>11181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>11.242599</td>\n",
       "      <td>13.244075</td>\n",
       "      <td>-72.576852</td>\n",
       "      <td>39.973146</td>\n",
       "      <td>-72.748974</td>\n",
       "      <td>40.006091</td>\n",
       "      <td>1.722118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>9.447462</td>\n",
       "      <td>6.548354</td>\n",
       "      <td>10.133452</td>\n",
       "      <td>5.777329</td>\n",
       "      <td>12.981577</td>\n",
       "      <td>5.664887</td>\n",
       "      <td>1.351062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-78.133333</td>\n",
       "      <td>-73.991278</td>\n",
       "      <td>-751.400000</td>\n",
       "      <td>-73.977970</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>6.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>-73.991849</td>\n",
       "      <td>40.734954</td>\n",
       "      <td>-73.991236</td>\n",
       "      <td>40.734008</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8.500000</td>\n",
       "      <td>14.000000</td>\n",
       "      <td>-73.981824</td>\n",
       "      <td>40.752640</td>\n",
       "      <td>-73.980164</td>\n",
       "      <td>40.753427</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.500000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>-73.967418</td>\n",
       "      <td>40.766700</td>\n",
       "      <td>-73.964153</td>\n",
       "      <td>40.767832</td>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>143.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>40.806487</td>\n",
       "      <td>41.366138</td>\n",
       "      <td>40.785400</td>\n",
       "      <td>41.366138</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        fare_amount     hourofday     pickuplon     pickuplat    dropofflon  \\\n",
       "count  11181.000000  11181.000000  11181.000000  11181.000000  11181.000000   \n",
       "mean      11.242599     13.244075    -72.576852     39.973146    -72.748974   \n",
       "std        9.447462      6.548354     10.133452      5.777329     12.981577   \n",
       "min        2.500000      0.000000    -78.133333    -73.991278   -751.400000   \n",
       "25%        6.000000      9.000000    -73.991849     40.734954    -73.991236   \n",
       "50%        8.500000     14.000000    -73.981824     40.752640    -73.980164   \n",
       "75%       12.500000     19.000000    -73.967418     40.766700    -73.964153   \n",
       "max      143.000000     23.000000     40.806487     41.366138     40.785400   \n",
       "\n",
       "         dropofflat    passengers  \n",
       "count  11181.000000  11181.000000  \n",
       "mean      40.006091      1.722118  \n",
       "std        5.664887      1.351062  \n",
       "min      -73.977970      0.000000  \n",
       "25%       40.734008      1.000000  \n",
       "50%       40.753427      1.000000  \n",
       "75%       40.767832      2.000000  \n",
       "max       41.366138      6.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_valid = bq.Query(query).execute().result().to_dataframe()\n",
    "df_valid.head()\n",
    "df_valid.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Create ML dataset using tf.transform and Dataflow\n",
    "\n",
    "Let's use Cloud Dataflow to read in the BigQuery data and write it out as CSV files. Along the way, let's use tf.transform to do scaling and transforming. Using tf.transform allows us to save the metadata to ensure that the appropriate transformations get carried out during prediction as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing requirements.txt\n"
     ]
    }
   ],
   "source": [
    "%writefile requirements.txt\n",
    "tensorflow-transform==0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "import apache_beam as beam\n",
    "import tensorflow_transform as tft\n",
    "from tensorflow_transform.beam import impl as beam_impl\n",
    "\n",
    "def is_valid(inputs):\n",
    "  try:\n",
    "    pickup_longitude = inputs['pickuplon']\n",
    "    dropoff_longitude = inputs['dropofflon']\n",
    "    pickup_latitude = inputs['pickuplat']\n",
    "    dropoff_latitude = inputs['dropofflat']\n",
    "    hourofday = inputs['hourofday']\n",
    "    dayofweek = inputs['dayofweek']\n",
    "    passenger_count = inputs['passengers']\n",
    "    fare_amount = inputs['fare_amount']\n",
    "    return (fare_amount >= 2.5 and pickup_longitude > -78 and pickup_longitude < -70 \\\n",
    "      and dropoff_longitude > -78 and dropoff_longitude < -70 and pickup_latitude > 37 \\\n",
    "      and pickup_latitude < 45 and dropoff_latitude > 37 and dropoff_latitude < 45 \\\n",
    "      and passenger_count > 0)\n",
    "  except:\n",
    "    return False\n",
    "  \n",
    "def preprocess_tft(inputs):\n",
    "      import datetime   \n",
    "      print inputs\n",
    "      result = {}\n",
    "      result['fare_amount'] = tf.identity(inputs['fare_amount'])     \n",
    "      result['dayofweek'] = tft.string_to_int(inputs['dayofweek']) # builds a vocabulary\n",
    "      result['hourofday'] = tf.identity(inputs['hourofday']) # pass through\n",
    "      result['pickuplon'] = (tft.scale_to_0_1(inputs['pickuplon'])) # scaling numeric values\n",
    "      result['pickuplat'] = (tft.scale_to_0_1(inputs['pickuplat']))\n",
    "      result['dropofflon'] = (tft.scale_to_0_1(inputs['dropofflon']))\n",
    "      result['dropofflat'] = (tft.scale_to_0_1(inputs['dropofflat']))\n",
    "      result['passengers'] = tf.cast(inputs['passengers'], tf.float32) # a cast\n",
    "      result['key'] = tf.as_string(tf.ones_like(inputs['passengers'])) # arbitrary TF func\n",
    "      # engineered features\n",
    "      latdiff = inputs['pickuplat'] - inputs['dropofflat']\n",
    "      londiff = inputs['pickuplon'] - inputs['dropofflon']\n",
    "      result['latdiff'] = tft.scale_to_0_1(latdiff)\n",
    "      result['londiff'] = tft.scale_to_0_1(londiff)\n",
    "      dist = tf.sqrt(latdiff * latdiff + londiff * londiff)\n",
    "      result['euclidean'] = dist\n",
    "      return result\n",
    "\n",
    "def preprocess(in_test_mode):\n",
    "  import os\n",
    "  import os.path\n",
    "  import tempfile\n",
    "  from apache_beam.io import tfrecordio\n",
    "  from tensorflow_transform.coders import example_proto_coder\n",
    "  from tensorflow_transform.tf_metadata import dataset_metadata\n",
    "  from tensorflow_transform.tf_metadata import dataset_schema\n",
    "  from tensorflow_transform.beam import tft_beam_io\n",
    "  from tensorflow_transform.beam.tft_beam_io import transform_fn_io\n",
    "\n",
    "  job_name = 'preprocess-taxi-features' + '-' + datetime.datetime.now().strftime('%y%m%d-%H%M%S')    \n",
    "  if in_test_mode:\n",
    "    import shutil\n",
    "    print 'Launching local job ... hang on'\n",
    "    OUTPUT_DIR = './preproc_tft'\n",
    "    shutil.rmtree(OUTPUT_DIR, ignore_errors=True)\n",
    "    EVERY_N = 100000\n",
    "  else:\n",
    "    print 'Launching Dataflow job {} ... hang on'.format(job_name)\n",
    "    OUTPUT_DIR = 'gs://{0}/taxifare/preproc_tft/'.format(BUCKET)\n",
    "    import subprocess\n",
    "    subprocess.call('gsutil rm -r {}'.format(OUTPUT_DIR).split())\n",
    "    EVERY_N = 10000\n",
    "    \n",
    "  options = {\n",
    "    'staging_location': os.path.join(OUTPUT_DIR, 'tmp', 'staging'),\n",
    "    'temp_location': os.path.join(OUTPUT_DIR, 'tmp'),\n",
    "    'job_name': job_name,\n",
    "    'project': PROJECT,\n",
    "    'max_num_workers': 24,\n",
    "    'teardown_policy': 'TEARDOWN_ALWAYS',\n",
    "    'no_save_main_session': True,\n",
    "    'requirements_file': 'requirements.txt'\n",
    "  }\n",
    "  opts = beam.pipeline.PipelineOptions(flags=[], **options)\n",
    "  if in_test_mode:\n",
    "    RUNNER = 'DirectRunner'\n",
    "  else:\n",
    "    RUNNER = 'DataflowRunner'\n",
    "\n",
    "  # set up metadata\n",
    "  raw_data_schema = {\n",
    "    colname : dataset_schema.ColumnSchema(tf.string, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in 'dayofweek,key'.split(',')\n",
    "  }\n",
    "  raw_data_schema.update({\n",
    "      colname : dataset_schema.ColumnSchema(tf.float32, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in 'fare_amount,pickuplon,pickuplat,dropofflon,dropofflat'.split(',')\n",
    "    })\n",
    "  raw_data_schema.update({\n",
    "      colname : dataset_schema.ColumnSchema(tf.int64, [], dataset_schema.FixedColumnRepresentation())\n",
    "                   for colname in 'hourofday,passengers'.split(',')\n",
    "    })\n",
    "  raw_data_metadata = dataset_metadata.DatasetMetadata(dataset_schema.Schema(raw_data_schema))\n",
    "\n",
    "  # run Beam  \n",
    "  with beam.Pipeline(RUNNER, options=opts) as p:\n",
    "    with beam_impl.Context(temp_dir=os.path.join(OUTPUT_DIR, 'tmp')):\n",
    "      # save the raw data metadata\n",
    "      _ = (raw_data_metadata\n",
    "        | 'WriteInputMetadata' >> tft_beam_io.WriteMetadata(\n",
    "            os.path.join(OUTPUT_DIR, 'metadata/rawdata_metadata'),\n",
    "            pipeline=p))\n",
    "      \n",
    "      # analyze and transform training       \n",
    "      raw_data = (p \n",
    "        | 'train_read' >> beam.io.Read(beam.io.BigQuerySource(query=create_query(1, EVERY_N), use_standard_sql=True))\n",
    "        | 'train_filter' >> beam.Filter(is_valid))\n",
    "\n",
    "      raw_dataset = (raw_data, raw_data_metadata)\n",
    "      transformed_dataset, transform_fn = (\n",
    "          raw_dataset | beam_impl.AnalyzeAndTransformDataset(preprocess_tft))\n",
    "      transformed_data, transformed_metadata = transformed_dataset\n",
    "      _ = transformed_data | 'WriteTrainData' >> tfrecordio.WriteToTFRecord(\n",
    "          os.path.join(OUTPUT_DIR, 'train'),\n",
    "          file_name_suffix='.gz',\n",
    "          coder=example_proto_coder.ExampleProtoCoder(\n",
    "              transformed_metadata.schema))\n",
    "      \n",
    "      # transform eval data\n",
    "      raw_test_data = (p \n",
    "        | 'eval_read' >> beam.io.Read(beam.io.BigQuerySource(query=create_query(2, EVERY_N), use_standard_sql=True))\n",
    "        | 'eval_filter' >> beam.Filter(is_valid))\n",
    "      \n",
    "      raw_test_dataset = (raw_test_data, raw_data_metadata)\n",
    "      transformed_test_dataset = (\n",
    "          (raw_test_dataset, transform_fn) | beam_impl.TransformDataset())\n",
    "      transformed_test_data, _ = transformed_test_dataset\n",
    "      _ = transformed_test_data | 'WriteTestData' >> tfrecordio.WriteToTFRecord(\n",
    "          os.path.join(OUTPUT_DIR, 'eval'),\n",
    "          file_name_suffix='.gz',\n",
    "          coder=example_proto_coder.ExampleProtoCoder(\n",
    "              transformed_metadata.schema))\n",
    "      _ = (transform_fn\n",
    "           | 'WriteTransformFn' >>\n",
    "           transform_fn_io.WriteTransformFn(os.path.join(OUTPUT_DIR, 'metadata')))\n",
    "\n",
    "preprocess(in_test_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "# ls preproc_tft\n",
    "gsutil ls -l gs://${BUCKET}/taxifare/preproc_tft/\n",
    "gsutil ls gs://${BUCKET}/taxifare/preproc_tft/metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "<h2> Train off preprocessed data </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "rm -rf taxifare_tft.tar.gz taxi_trained\n",
    "export PYTHONPATH=${PYTHONPATH}:$PWD/taxifare_tft\n",
    "python -m trainer.task \\\n",
    "   --train_data_paths=\"gs://${BUCKET}/taxifare/preproc_tft/train*\" \\\n",
    "   --eval_data_paths=\"gs://${BUCKET}/taxifare/preproc_tft/eval*\"  \\\n",
    "   --output_dir=./taxi_trained \\\n",
    "   --train_steps=10 --job-dir=/tmp \\\n",
    "   --metadata_path=gs://${BUCKET}/taxifare/preproc_tft/metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "!ls $PWD/taxi_trained/export/exporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%writefile /tmp/test.json\n",
    "{\"dayofweek\":\"Thu\",\"hourofday\":17,\"pickuplon\": -73.885262,\"pickuplat\": 40.773008,\"dropofflon\": -73.987232,\"dropofflat\": 40.732403,\"passengers\": 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "%bash\n",
    "model_dir=$(ls $PWD/taxi_trained/export/exporter/)\n",
    "gcloud ml-engine local predict \\\n",
    "    --model-dir=./taxi_trained/export/exporter/${model_dir} \\\n",
    "    --json-instances=/tmp/test.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Copyright 2016-2018 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
